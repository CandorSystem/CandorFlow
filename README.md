# ğŸŒŠ CandorFlow

**Early Warning System for Training Instabilities**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

---

## âš ï¸ Important Notice

**This repository contains a SIMPLIFIED, PUBLIC DEMONSTRATION of CandorFlow concepts.**

This is NOT the full proprietary system. Many advanced features, algorithms, and optimizations are intentionally excluded. See [What Is NOT Included](#what-is-not-included-proprietary) for details.

---

## ğŸ“– Overview

CandorFlow is a training stability monitoring and intervention system designed to detect and prevent neural network training instabilities before they cause divergence.

This public repository demonstrates:
- A simplified stability metric **Î»(t)** based on gradient variance
- Basic threshold-based monitoring
- Automatic checkpoint rollback on instability detection
- Learning rate reduction for recovery
- Minimal working examples with toy models

### What is Î»(t)?

The lambda metric **Î»(t)** is a stability indicator that tracks training health over time. In this simplified demo, it measures gradient norm variance as a proxy for instability.

**High Î»(t) â†’ Training is becoming unstable**  
**Low Î»(t) â†’ Training is stable**

---

## ğŸ¯ Features in This Demo

### âœ… What This Repo Contains (Safe/Public Demo)

- **Simplified Î»(t) metric**: Gradient norm variance-based instability detection
- **Basic stability controller**: Threshold monitoring with rollback capabilities
- **Checkpoint management**: Automatic saving and restoration
- **Learning rate adaptation**: Halving on instability detection
- **Minimal training loop**: Toy example with intentional instability
- **Visualization tools**: Plot Î»(t) curves and stability phases
- **Jupyter notebook**: Interactive demo with explanations
- **Reproducible examples**: Fully runnable on CPU or GPU

---

## ğŸš« What Is NOT Included (Proprietary)

The full CandorFlow system includes many advanced features that are **NOT** in this public demo:

### Core Algorithms
- âŒ **Universal scaling law** for Î»(t)
- âŒ **Reflexive ridge equation** and closed-form solutions
- âŒ **Cross-domain invariants** (works across NLP, vision, RL, etc.)
- âŒ **Jacobian spectral analysis** for stability prediction
- âŒ **Multi-signal fusion** (loss, gradients, activations, etc.)

### Advanced Control
- âŒ **Real-time stability engine** with predictive modeling
- âŒ **Reflexive decay algorithms** for adaptive intervention
- âŒ **Temporal smoothing with active inference**
- âŒ **Dynamic threshold adaptation** based on training phase
- âŒ **HPC-optimized control loops** for large-scale training

### Domain Extensions
- âŒ **ECG anomaly detection** applications
- âŒ **Earthquake early warning** systems
- âŒ **Financial market stability** monitoring
- âŒ **General-purpose time series** instability detection

### Performance
- âŒ **Production-grade optimizations** for minimal overhead
- âŒ **Distributed training integration** (DeepSpeed, FSDP, etc.)
- âŒ **Hardware acceleration** (CUDA kernels, etc.)

**For access to the full proprietary system, please contact us.**

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Install Dependencies

```bash
pip install -r requirements.txt
```

Or install manually:

```bash
pip install torch transformers numpy matplotlib jupyter notebook
```

---

## ğŸ’» Usage

### Quick Start: Run the Training Demo

```bash
python examples/demo_training_loop.py
```

This will:
1. Create a small neural network
2. Train it on dummy data
3. Compute Î»(t) at each step
4. Intentionally inject instability after step 30
5. Demonstrate automatic detection and rollback
6. Save results to `plots/training_results.pt`

**Expected output:**
```
================================================================
CandorFlow Training Stability Demo
================================================================
NOTE: This is a simplified demonstration.
The full proprietary system includes many additional features.
================================================================

ğŸ“¦ Creating model...
ğŸ”§ Initializing stability controller...
ğŸš€ Starting training for 50 steps...

Step   0 | Loss: 0.6931 | Î»(t): 0.0000 | Action: none
Step   5 | Loss: 0.6895 | Î»(t): 0.0234 | Action: none
...
âš ï¸  INSTABILITY DETECTED at step 35: Î»(t)=3.4521 (threshold=2.0)
âœ“ Rolled back to stable checkpoint from step 25
âœ“ Reduced learning rate: 0.001000 â†’ 0.000500
```

### Visualize Results

```bash
python examples/demo_plots.py
```

This generates:
- `plots/lambda_curve.png` - Î»(t) over time with intervention markers
- `plots/stability_phases.png` - Color-coded stability zones

### Interactive Notebook

```bash
jupyter notebook notebooks/CandorFlow_Demo.ipynb
```

The notebook includes:
- Step-by-step explanations
- Live training visualization
- Interactive parameter tuning
- Educational content about stability monitoring

---

## ğŸ“ Repository Structure

```
CandorFlow/
â”‚
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ LICENSE                     # MIT License
â”‚
â”œâ”€â”€ candorflow/                 # Main package
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ lambda_metric.py       # Simplified Î»(t) computation
â”‚   â”œâ”€â”€ stability_controller.py # Basic monitoring & intervention
â”‚   â”œâ”€â”€ utils.py               # Checkpoint and logging utilities
â”‚   â””â”€â”€ version.py             # Version information
â”‚
â”œâ”€â”€ examples/                   # Runnable demos
â”‚   â”œâ”€â”€ demo_training_loop.py  # Training with stability monitoring
â”‚   â””â”€â”€ demo_plots.py          # Visualization generation
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”‚   â””â”€â”€ CandorFlow_Demo.ipynb  # Interactive tutorial
â”‚
â””â”€â”€ plots/                      # Output directory for plots
    â””â”€â”€ (generated files)
```

---

## ğŸ”¬ How It Works (Simplified Version)

### 1. Monitor Training with Î»(t)

```python
from candorflow import compute_lambda_metric, StabilityController

# During training loop
lambda_value = compute_lambda_metric(
    model=model,
    loss=loss,
    gradient_history=gradient_history
)
```

### 2. Automatic Intervention

```python
controller = StabilityController(threshold=2.0)

action = controller.update(
    lambda_value=lambda_value,
    model=model,
    optimizer=optimizer,
    step=step
)

if action["action"] == "rollback":
    print("Instability detected - rolling back to stable checkpoint")
```

### 3. Training Continues Safely

The controller automatically:
- Saves checkpoints when training is stable
- Detects when Î»(t) exceeds threshold
- Rolls back to last stable state
- Reduces learning rate
- Resumes training

---

## ğŸ“Š Example Results

After running the demo, you'll see plots like this:

**Lambda Curve with Interventions:**
- Blue line: Î»(t) stability metric over time
- Purple dashed line: Instability threshold
- Orange markers: Rollback + LR reduction events
- Red markers: Warnings

**Stability Phases:**
- Green zone: Stable training
- Orange zone: Warning (approaching threshold)
- Red zone: Unstable (intervention triggered)

---

## ğŸ§ª Running Tests

The demo includes built-in validation:

```bash
# Run training demo (includes self-checks)
python examples/demo_training_loop.py

# Generate plots (validates results)
python examples/demo_plots.py
```

---

## ğŸ“š Documentation

### API Reference

#### `compute_lambda_metric(model, loss, history_window=10, gradient_history=None)`

Compute simplified Î»(t) stability metric.

**Parameters:**
- `model` (torch.nn.Module): Neural network model
- `loss` (torch.Tensor): Current loss value (with grad_fn)
- `history_window` (int): Number of past gradient norms to track
- `gradient_history` (list): List to store gradient history (modified in-place)

**Returns:**
- `lambda_value` (float): Stability metric (higher = more unstable)

#### `StabilityController(threshold, checkpoint_dir, lr_reduction_factor)`

Training stability monitor and intervention system.

**Parameters:**
- `threshold` (float): Î»(t) value above which to trigger intervention
- `checkpoint_dir` (str): Directory for saving checkpoints
- `lr_reduction_factor` (float): Factor to reduce LR by (default: 0.5)

**Methods:**
- `update(lambda_value, model, optimizer, step)`: Update controller and take action if needed
- `get_summary()`: Get training statistics

---

## ğŸ¤ Contributing

This is a demonstration repository. Contributions are welcome for:
- Bug fixes in demo code
- Documentation improvements
- Additional visualization examples
- Educational content

**Note:** This repo intentionally excludes proprietary algorithms. Please do not submit PRs attempting to implement advanced features from the full system.

---

## ğŸ“§ Contact

For questions about this demo:
- Open an issue on GitHub

For inquiries about the full proprietary CandorFlow system:
- Email: [your-email@example.com]
- Website: [https://candorflow.example.com]
- Patents: [Patent application numbers]

---

## ğŸ“„ License

This simplified demonstration code is released under the MIT License. See [LICENSE](LICENSE) for details.

**Important:** The full CandorFlow system, including its proprietary algorithms and commercial applications, is NOT covered by this license. Please contact us for commercial licensing.

---

## ğŸ“– Citation

If you use this demo code in your research or project, please cite:

```bibtex
@software{candorflow2025,
  title={CandorFlow: Training Stability Monitoring System},
  author={[Your Name]},
  year={2025},
  url={https://github.com/yourusername/CandorFlow},
  note={Simplified public demonstration version}
}
```

---

## ğŸ™ Acknowledgments

This simplified demo is provided for educational purposes to demonstrate basic concepts in training stability monitoring.

The full CandorFlow system represents significant research and development investment and is protected by pending patents.

---

## â­ Star History

If you find this demo helpful, please consider starring the repository!

---

**Built with â¤ï¸ for safer, more reliable AI training**

